{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kkim\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Neurosynth practice example \n",
    "2018_07_29\n",
    "Kun Il Kim\n",
    "\"\"\"\n",
    "# ## Importing modules\n",
    "\"\"\"\n",
    " Let's start with the basics. In Python, most modules (i.e., organized chunks of code) are \n",
    "inaccessible by default. Other than a few very basic built-in functions, \n",
    "you'll need to explicitly include every piece of code you want to work with. \n",
    "This may seem cumbersome, but it has the nice effect of (a) making sure you always know exactly \n",
    "what dependencies your code has, and (b) minimizing the memory footprint of your app \n",
    "by only including functionality you know you'll need.\n",
    " \n",
    " Like most Python packages, Neurosynth consists of several modules arranged into \n",
    " a semi-sensible tree structure. For this lab, we'll need functionality available \n",
    " in several modules, which we can include like so:\n",
    "\"\"\"\n",
    "from neurosynth.base.dataset import Dataset\n",
    "from neurosynth.analysis import meta, decode, network\n",
    "import neurosynth\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " ## Creating a new dataset\n",
    "\n",
    " Next, we create a Dataset, which is the core object most Neurosynth tools operate on. We initialize a Dataset by passing in a database file, which is essentially just a giant list of activation coordinates and associated study IDs. This file can be downloaded from the Neurosynth website or installed from the data submodule (see the Readme for instructions).\n",
    " \n",
    " Creating the object will take a few minutes on most machines, as we need to process about 200,000 activations drawn from nearly 6,000 studies. Once that's done, we also need to add some features to the Dataset. Features are just variables associated with the studies in our dataset; literally any dimension a study could be coded on can constitute a feature that Neurosynth can use. In practice, the default set of features included in the data download includes 500 psychological terms (e.g., 'language', 'emotion', 'memory', etc.) that occur with some frequency in the dataset. So when we're talking about the \"emotion\" feature, we're really talking about how frequently each study in the Dataset uses the word 'emotion' in the full-text of the corresponding article.\n",
    " \n",
    " Let's go ahead and create a dataset and add some features:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This operation takes a long time. Only do it single time\n",
    "\n",
    "dataset = Dataset('database.txt')\n",
    "dataset.add_features('features.txt')\n",
    "\n",
    "# Because this takes a while, we'll save our Dataset object to disk. \n",
    "#That way, the next time we want to use it, we won't have to sit through \n",
    "#the whole creation operation again:\n",
    "dataset.save('dataset.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.load('data/dataset.pkl')   # Note the capital D in the second Dataset--load() is a class method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids1 = dataset.get_studies(features='theory mind', frequency_threshold=0.05)\n",
    "ids2 = dataset.get_studies(features='emotion', frequency_threshold=0.05)\n",
    "ids3 = dataset.get_studies(features='valence', frequency_threshold=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175\n",
      "901\n",
      "358\n"
     ]
    }
   ],
   "source": [
    "print(len(ids1))\n",
    "print(len(ids2))\n",
    "print(len(ids3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kkim\\Desktop\\neurosynth_project\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "print(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kkim\\Anaconda3\\lib\\site-packages\\neurosynth\\analysis\\meta.py:134: RuntimeWarning: invalid value encountered in true_divide\n",
      "  pFgA = pAgF * pF / pA\n",
      "C:\\Users\\kkim\\Anaconda3\\lib\\site-packages\\neurosynth\\analysis\\meta.py:139: RuntimeWarning: invalid value encountered in true_divide\n",
      "  pFgA_prior = pAgF * prior / pAgF_prior\n",
      "Generating LALR tables\n",
      "WARNING: 15 shift/reduce conflicts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1711 studies.\n"
     ]
    }
   ],
   "source": [
    "# Run a meta-analysis on emotion\n",
    "ids = dataset.get_studies('emo*', frequency_threshold=0.05)\n",
    "ma = meta.MetaAnalysis(dataset, ids)\n",
    "ma.save_results('.', 'emotion') \n",
    "\n",
    "ids = dataset.get_studies(expression='emo* &~ (reward* | pain*)', frequency_threshold=0.05)\n",
    "ma = meta.MetaAnalysis(dataset, ids)\n",
    "ma.save_results('.', 'emotion_without_reward_or_pain')\n",
    "print(\"Found %d studies.\" %len(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating LALR tables\n",
      "WARNING: 15 shift/reduce conflicts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1300 studies.\n"
     ]
    }
   ],
   "source": [
    "ids = dataset.get_studies(expression='emotion | valence | theory mind', frequency_threshold=0.05)\n",
    "ma = meta.MetaAnalysis(dataset, ids)\n",
    "ma.save_results('.', 'emotion valence theory mind')\n",
    "print(\"Found %d studies.\" %len(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating LALR tables\n",
      "WARNING: 15 shift/reduce conflicts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 60 studies.\n"
     ]
    }
   ],
   "source": [
    "ids = dataset.get_studies(expression='punishment', frequency_threshold=0.1)\n",
    "ma = meta.MetaAnalysis(dataset, ids)\n",
    "ma.save_results('.', 'punishment')\n",
    "print(\"Found %d studies.\" % len(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating LALR tables\n",
      "WARNING: 15 shift/reduce conflicts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1761 studies.\n"
     ]
    }
   ],
   "source": [
    "ids = dataset.get_studies(expression='(decis* | valu* | mentali*  )', frequency_threshold=0.05)\n",
    "ma = meta.MetaAnalysis(dataset, ids)\n",
    "ma.save_results('.', 'process_1')\n",
    "print(\"Found %d studies.\" % len(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating LALR tables\n",
      "WARNING: 15 shift/reduce conflicts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1824 studies.\n"
     ]
    }
   ],
   "source": [
    "ids = dataset.get_studies(expression='(decis* | valu* | empath*  )', frequency_threshold=0.05)\n",
    "ma = meta.MetaAnalysis(dataset, ids)\n",
    "ma.save_results('.', 'process_2')\n",
    "print(\"Found %d studies.\" % len(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meta-analytic contrasts\n",
    "\n",
    "\n",
    "In addition to various logical operations, one handy thing you can do with Neurosynth is perform meta-analytic contrasts. Meaning, you can identify voxels in which the average likelihood of activation being reported differ for two different sets of studies. For example, let's say you want to meta-analytically contrast studies that use the term 'recollection' with studies that use the term 'recognition'. You can do this by defining both sets of studies separately, and then passing them both to the meta-analysis object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We found 986 studies of self\n",
      "We found 1098 studies of other\n"
     ]
    }
   ],
   "source": [
    "# Get the recognition studies and print some info...\n",
    "cont1_ids = dataset.get_studies('self', frequency_threshold=0.05)\n",
    "print(\"We found %d studies of self\" % len(cont1_ids))\n",
    "\n",
    "# Repeat for recollection studies\n",
    "cont2_ids = dataset.get_studies('social', frequency_threshold=0.05)\n",
    "print(\"We found %d studies of other\" % len(cont2_ids))\n",
    "\n",
    "# Run the meta-analysis\n",
    "ma = meta.MetaAnalysis(dataset, cont1_ids, cont2_ids)\n",
    "ma.save_results('.', 'self vs social')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kkim\\Anaconda3\\lib\\site-packages\\pandas\\core\\sparse\\frame.py:700: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  sdict = dict((k, v) for k, v in compat.iteritems(self) if k in columns)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'SparseDataFrame' objects are mutable, thus they cannot be hashed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-72f036c9e8fa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Get the recognition studies and print some info...\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcont1_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_studies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'(decis* | valu* | mentali*  )'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfrequency_threshold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.05\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"We found %d studies of cond1\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcont1_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Repeat for recollection studies\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\neurosynth\\base\\dataset.py\u001b[0m in \u001b[0;36mget_studies\u001b[1;34m(self, features, expression, mask, peaks, frequency_threshold, activation_threshold, func, return_type, r)\u001b[0m\n\u001b[0;32m    332\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    333\u001b[0m                 results.append(self.feature_table.get_ids(\n\u001b[1;32m--> 334\u001b[1;33m                     features, frequency_threshold, func))\n\u001b[0m\u001b[0;32m    335\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m         \u001b[1;31m# Logical expression-based selection\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\neurosynth\\base\\dataset.py\u001b[0m in \u001b[0;36mget_ids\u001b[1;34m(self, features, threshold, func, get_weights)\u001b[0m\n\u001b[0;32m    716\u001b[0m         \u001b[0mfeature_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    717\u001b[0m         \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeature_weights\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 718\u001b[1;33m         \u001b[0mabove_thresh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mweights\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    719\u001b[0m         \u001b[1;31m# ids_to_keep = self.ids[above_thresh]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    720\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mabove_thresh\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mget_weights\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mabove_thresh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\sparse\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    430\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 432\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    433\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtakeable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m   1838\u001b[0m         \u001b[1;34m\"\"\"Return the cached item, item represents a label indexer.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1839\u001b[0m         \u001b[0mcache\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_item_cache\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1840\u001b[1;33m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1841\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1842\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__hash__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1043\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__hash__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1044\u001b[0m         raise TypeError('{0!r} objects are mutable, thus they cannot be'\n\u001b[1;32m-> 1045\u001b[1;33m                         ' hashed'.format(self.__class__.__name__))\n\u001b[0m\u001b[0;32m   1046\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1047\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'SparseDataFrame' objects are mutable, thus they cannot be hashed"
     ]
    }
   ],
   "source": [
    "# Get the recognition studies and print some info...\n",
    "cont1_ids = dataset.get_studies('(decis* | valu* | mentali*  )', frequency_threshold=0.05)\n",
    "print(\"We found %d studies of cond1\" % len(cont1_ids))\n",
    "\n",
    "# Repeat for recollection studies\n",
    "cont2_ids = dataset.get_studies('(decis* | valu* | empath*  )', frequency_threshold=0.05)\n",
    "print(\"We found %d studies of cond2\" % len(cont2_ids))\n",
    "\n",
    "# Run the meta-analysis\n",
    "ma = meta.MetaAnalysis(dataset, cont1_ids, cont2_ids)\n",
    "ma.save_results('.', 'cond1 vs cond2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We found 1098 studies of self\n",
      "We found 986 studies of other\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kkim\\Anaconda3\\lib\\site-packages\\neurosynth\\analysis\\meta.py:134: RuntimeWarning: invalid value encountered in true_divide\n",
      "  pFgA = pAgF * pF / pA\n",
      "C:\\Users\\kkim\\Anaconda3\\lib\\site-packages\\neurosynth\\analysis\\meta.py:139: RuntimeWarning: invalid value encountered in true_divide\n",
      "  pFgA_prior = pAgF * prior / pAgF_prior\n"
     ]
    }
   ],
   "source": [
    "# Get the recognition studies and print some info...\n",
    "cont1_ids = dataset.get_studies('social', frequency_threshold=0.05)\n",
    "print(\"We found %d studies of self\" % len(cont1_ids))\n",
    "\n",
    "# Repeat for recollection studies\n",
    "cont2_ids = dataset.get_studies('self', frequency_threshold=0.05)\n",
    "print(\"We found %d studies of other\" % len(cont2_ids))\n",
    "\n",
    "# Run the meta-analysis\n",
    "ma = meta.MetaAnalysis(dataset, cont1_ids, cont2_ids)\n",
    "ma.save_results('.', 'social-self')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seed-based coactivation maps\n",
    "\n",
    "By now you're all familiar with seed-based functional connectivity. We can do something very similar at a meta-analytic level (e.g., Toro et al, 2008, Robinson et al, 2010, Chang et al, 2012) using the Neurosynth data. Specifically, we can define a seed region and then ask what other regions tend to be reported in studies that report activity in our seed region. The Neurosynth tools make this very easy to do. We can either pass in a mask image defining our ROI, or pass in a list of coordinates to use as the centroid of spheres. In this example, we'll do the latter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed-based coactivation\n",
    "#network.coactivation(dataset, [[3, 49, 31]], threshold=0.1, r=10, output_dir='.', prefix='Admpfc_seed')\n",
    "#network.coactivation(dataset, [[2, 24, 46]], threshold=0.1, r=10, output_dir='.', prefix='Pdmpfc_seed')\n",
    "network.coactivation(dataset, [[2, 44, 12]], threshold=0.1, r=10, output_dir='.', prefix='avdmpfc_seed')\n",
    "network.coactivation(dataset, [[4, 52, 8]], threshold=0.1, r=10, output_dir='.', prefix='mmpfc_seed')\n",
    "network.coactivation(dataset, [[0, 56, 2]], threshold=0.1, r=10, output_dir='.', prefix='vmpfc_seed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.coactivation(dataset, [[2, 34, -6]], threshold=0.1, r=10, output_dir='.', prefix='4_1')\n",
    "network.coactivation(dataset, [[2, 34, 4]], threshold=0.1, r=10, output_dir='.',prefix='2_1')\n",
    "network.coactivation(dataset, [[2, 34, 14]], threshold=0.1, r=10, output_dir='.', prefix='2_2')\n",
    "network.coactivation(dataset, [[2, 34, 24]], threshold=0.1, r=10, output_dir='.', prefix='12_1')\n",
    "network.coactivation(dataset, [[2, 34, 34]], threshold=0.1, r=10, output_dir='.', prefix='12_2')\n",
    "network.coactivation(dataset, [[2, 34, 44]], threshold=0.1, r=10, output_dir='.', prefix='11_1')\n",
    "network.coactivation(dataset, [[2, 34, 54]], threshold=0.1, r=10, output_dir='.', prefix='11_2')\n",
    "network.coactivation(dataset, [[2, 44, -16]], threshold=0.1, r=10, output_dir='.', prefix='4_2')\n",
    "network.coactivation(dataset, [[2, 44, -6]], threshold=0.1, r=10, output_dir='.', prefix='4_3')\n",
    "network.coactivation(dataset, [[2, 44, 4]], threshold=0.1, r=10, output_dir='.', prefix='2_3')\n",
    "network.coactivation(dataset, [[2, 44, 14]], threshold=0.1, r=10, output_dir='.', prefix='2_4')\n",
    "network.coactivation(dataset, [[2, 44, 24]], threshold=0.1, r=10, output_dir='.', prefix='12_3')\n",
    "network.coactivation(dataset, [[2, 44, 34]], threshold=0.1, r=10, output_dir='.', prefix='11_3')\n",
    "network.coactivation(dataset, [[2, 44, 44]], threshold=0.1, r=10, output_dir='.', prefix='11_4')\n",
    "network.coactivation(dataset, [[2, 54, -6]], threshold=0.1, r=10, output_dir='.', prefix='4_4')\n",
    "network.coactivation(dataset, [[2, 34, 4]], threshold=0.1, r=10, output_dir='.', prefix='2_5')\n",
    "network.coactivation(dataset, [[2, 54, 14]], threshold=0.1, r=10, output_dir='.', prefix='6_1')\n",
    "network.coactivation(dataset, [[2, 54, 24]], threshold=0.1, r=10, output_dir='.', prefix='6_2')\n",
    "network.coactivation(dataset, [[2, 54, 34]], threshold=0.1, r=10, output_dir='.', prefix='6_3')\n",
    "network.coactivation(dataset, [[2, 64, -6]], threshold=0.1, r=10, output_dir='.', prefix='4_5')\n",
    "network.coactivation(dataset, [[2, 64, 4]], threshold=0.1, r=10, output_dir='.', prefix='6_4')\n",
    "network.coactivation(dataset, [[2, 64, 14]], threshold=0.1, r=10, output_dir='.', prefix='6_5')\n",
    "network.coactivation(dataset, [[2, 64, 24]], threshold=0.1, r=10, output_dir='.', prefix='6_6')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_keywords = ['reward', 'anticipation', 'monetary', 'responses', 'motivation', 'loss', 'incentive', 'punishment', 'gain', 'outcome',\n",
    "                 'social', 'empathy', 'moral', 'person', 'judgments', 'mentalizing', 'theory', 'perspective', 'cognition', 'mind',\n",
    "                 'memory', 'events', 'imagery', 'autobiographical', 'retrieval', 'episodic', 'memories', 'future', 'semantic', 'past', 'event', \n",
    "                'decision', 'choice', 'risk', 'decisions', 'uncertainty', 'risky', 'taking', 'behavior', 'preference', 'probability',\n",
    "                'inhibition', 'control', 'stop', 'motor', 'trials', 'nogo', 'cognitive', 'suppression', 'aggression', 'error', \n",
    "                'pain', 'stimulation', 'somatosensory', 'intensity', 'noxious', 'heat', 'nociceptive', 'placebo', 'chronic', 'sensory']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_keywords2 = ['eye', 'gaze', 'movements', 'eyes', 'visual', 'saccades', 'saccade', 'target', 'fixation', 'direction',\n",
    "                 'decision','choice', 'risk', 'decisions', 'choices', 'uncertainty', 'outcomes', 'risky', 'taking', 'outcome', \n",
    "                 'memory', 'events', 'imagery', 'autobiographical', 'retrieval', 'episodic' 'memories' 'future' 'mental' 'semantic',\n",
    "                 'motor', 'movement', 'movements', 'sensorimotor', 'primary', 'finger', 'control', 'imagery', 'tasks', 'force', \n",
    "                 'social', 'empathy', 'moral', 'person' 'judgments', 'mentalizing', 'mental', 'theory', 'people', 'mind', \n",
    "                 'reward', 'anticipation', 'monetary', 'responses', 'rewards', 'motivation', 'motivational', 'loss', 'incentive', 'punishment',\n",
    "                 'cues','target', 'trials', 'cue', 'switching', 'stimulus' 'targets', 'preparation', 'switch', 'selection',\n",
    "                 'conflict', 'interference', 'control', 'incongruent', 'trials', 'stroop', 'congruent', 'cognitive', 'behavioral', 'rt',\n",
    "                 'inhibition', 'control', 'inhibitory', 'stop' 'motor', 'trials', 'nogo', 'cognitive', 'suppression', 'aggression', \n",
    "                 'fear anxiety', 'threat', 'responses', 'conditioning', 'cs', 'extinction', 'autonomic', 'conditioned', 'arousal',\n",
    "                 'memory', 'performance', 'cognitive', 'wm', 'tasks', 'verbal', 'load', 'executive', 'test', 'maintenance',\n",
    "                 'pain', 'painful', 'stimulation', 'somatosensory', 'intensity', 'noxious', 'heat', 'nociceptive', 'placebo', 'chronic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode images\n",
    "\n",
    "decoder = decode.Decoder(dataset, features=feat_keywords2)\n",
    "data = decoder.decode(['6_6_uniformity-test_z_FDR_0.01.nii.gz',\n",
    "                      '6_5_uniformity-test_z_FDR_0.01.nii.gz',\n",
    "                      '6_4_uniformity-test_z_FDR_0.01.nii.gz',\n",
    "                      '6_3_uniformity-test_z_FDR_0.01.nii.gz',\n",
    "                      '6_2_uniformity-test_z_FDR_0.01.nii.gz',\n",
    "                      '6_1_uniformity-test_z_FDR_0.01.nii.gz',\n",
    "                      '2_5_uniformity-test_z_FDR_0.01.nii.gz',\n",
    "                      '2_4_uniformity-test_z_FDR_0.01.nii.gz',\n",
    "                      '2_3_uniformity-test_z_FDR_0.01.nii.gz',\n",
    "                      '2_2_uniformity-test_z_FDR_0.01.nii.gz',\n",
    "                      '2_1_uniformity-test_z_FDR_0.01.nii.gz',\n",
    "                      '11_4_uniformity-test_z_FDR_0.01.nii.gz',\n",
    "                      '11_3_uniformity-test_z_FDR_0.01.nii.gz',\n",
    "                      '11_2_uniformity-test_z_FDR_0.01.nii.gz',\n",
    "                      '11_1_uniformity-test_z_FDR_0.01.nii.gz',\n",
    "                      '4_5_uniformity-test_z_FDR_0.01.nii.gz',\n",
    "                      '4_4_uniformity-test_z_FDR_0.01.nii.gz',\n",
    "                      '4_3_uniformity-test_z_FDR_0.01.nii.gz',\n",
    "                      '4_2_uniformity-test_z_FDR_0.01.nii.gz',\n",
    "                      '4_1_uniformity-test_z_FDR_0.01.nii.gz',\n",
    "                      '12_1_uniformity-test_z_FDR_0.01.nii.gz',\n",
    "                      '12_2_uniformity-test_z_FDR_0.01.nii.gz',\n",
    "                       '12_3_uniformity-test_z_FDR_0.01.nii.gz'\n",
    "                      ], save='manu_keywords_decoding_results.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
